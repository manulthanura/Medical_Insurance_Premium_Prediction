# -*- coding: utf-8 -*-
"""Medical Insurance Premium Prediction with Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nOJYnPgpLXmUiLxSoHRQfWxAaO7EieYs

# TASK #1: UNDERSTAND THE PROBLEM STATEMENT

Predict medical insurance cost with machine learning. The objective of this case study is to predict the health insurance cost incurred by Individuals based on their age, gender, Body Mass Index (BMI), number of children, smoking habits, and geo-location.

# TASK #2: IMPORT LIBRARIES AND DATASETS
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# optional

# from jupyterthemes import jtplot
# jtplot.style(theme = 'monokai', context = 'notebook', ticks = True, grid = False)

# setting the style of the notebook to be monokai theme
# this line of code is important to ensure that we are able to see the x and y axes clearly
# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them.

# read the csv file
# insurance_df = pd.read_csv('insurance.csv')
insurance_df = pd.read_csv('/content/sample_data/insurance.csv') # Google Colab

insurance_df

insurance_df.info()

"""# TASK #3: PERFORM EXPLORATORY DATA ANALYSIS - PART 1"""

insurance_df.isnull()

insurance_df.isnull().sum()

# check if there are any Null values
sns.heatmap(insurance_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")

# check if there are any Null values
insurance_df.isnull().sum()

# Check the dataframe info
insurance_df.info()

insurance_df.describe()

# Grouping by region to see any relationship between region and charges
# Seems like south east region has the highest charges and body mass index

df_region = insurance_df.groupby(by = 'region').mean()
df_region

# Group data by 'age' and examine the relationship between 'age' and 'charges'

df_age = insurance_df.groupby(by = 'age').mean()
df_age

"""# TASK #4: PERFORM FEATURE ENGINEERING"""

# Check unique values in the 'sex' column
insurance_df['sex'].unique()

# convert categorical variable to numerical
insurance_df['sex'] = insurance_df['sex'].apply(lambda x: 0 if x == 'female' else 1)

insurance_df.head()

# Check the unique values in the 'smoker' column
insurance_df['smoker'].unique()

# Convert categorical variable to numerical
insurance_df['smoker'] = insurance_df['smoker'].apply(lambda x: 0 if x == 'no' else 1)

insurance_df.head()

# Check unique values in 'region' column
insurance_df['region'].unique()

region_dummies = pd.get_dummies(insurance_df['region'], drop_first = True)

region_dummies

insurance_df = pd.concat([insurance_df, region_dummies], axis = 1)

insurance_df.head()

# Let's drop the original 'region' column
insurance_df.drop(['region'], axis = 1, inplace = True)

insurance_df.head()

"""# TASK #5: PERFORM DATA VISUALIZATION"""

insurance_df[['age','sex','bmi','children','smoker','charges']].hist(bins = 30, figsize = (12,12))

# plot pairplot
sns.pairplot(insurance_df)

plt.figure(figsize = (15, 6))
sns.regplot(x = 'age', y = 'charges', data = insurance_df)
plt.show()

plt.figure(figsize = (15, 6))
sns.regplot(x = 'bmi', y = 'charges', data = insurance_df)
plt.show()

# Calculate and plot the correlation matrix
insurance_df.corr()

plt.figure(figsize = (10, 10))
sns.heatmap(insurance_df.corr(), annot = True)

"""# TASK #6: CREATE TRAINING AND TESTING DATASET"""

insurance_df.columns

X = insurance_df.drop(columns = ['charges'])
y = insurance_df['charges']

X

y

X.shape

y.shape

X = np.array(X).astype('float32')
y = np.array(y).astype('float32')

y = y.reshape(-1,1)

# Only take the numerical variables and scale them
X

y.shape

#scaling the data before feeding the model
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler_x = StandardScaler()
X = scaler_x.fit_transform(X)

scaler_y = StandardScaler()
y = scaler_y.fit_transform(y)

# Split the data into 20% Testing and 80% Training
# Double check that the split was successful by getting the shape of both the training and testing datasets

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =10)

print(X_train.shape)
print(X_test.shape)

"""# TASK #7: TRAIN AND EVALUATE A LINEAR REGRESSION MODEL IN SCIKIT-LEARN"""

# using linear regression model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, accuracy_score

regresssion_model_sklearn = LinearRegression()
regresssion_model_sklearn.fit(X_train, y_train)

regresssion_model_sklearn_accuracy = regresssion_model_sklearn.score(X_test, y_test)
regresssion_model_sklearn_accuracy

y_predict = regresssion_model_sklearn.predict(X_test)

y_predict_orig = scaler_y.inverse_transform(y_predict)
y_test_orig = scaler_y.inverse_transform(y_test)

k = X_test.shape[1]
n = len(X_test)
n

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt


RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)),'.3f'))
MSE = mean_squared_error(y_test_orig, y_predict_orig)
MAE = mean_absolute_error(y_test_orig, y_predict_orig)
r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)

"""# TASK #8: TRAIN AND EVALUATE AN ARTIFICIAL NEURAL NETWORK-BASED REGRESSION MODEL"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

ANN_model = keras.Sequential()
ANN_model.add(Dense(50, input_dim = 8))
ANN_model.add(Activation('relu'))
ANN_model.add(Dense(150))
ANN_model.add(Activation('relu'))
ANN_model.add(Dropout(0.5))
ANN_model.add(Dense(150))
ANN_model.add(Activation('relu'))
ANN_model.add(Dropout(0.5))
ANN_model.add(Dense(50))
ANN_model.add(Activation('linear'))
ANN_model.add(Dense(1))
ANN_model.compile(loss = 'mse', optimizer = 'adam')
ANN_model.summary()

ANN_model.compile(optimizer='Adam', loss='mean_squared_error')
epochs_hist = ANN_model.fit(X_train, y_train, epochs = 100, batch_size = 20, validation_split = 0.2)

result = ANN_model.evaluate(X_test, y_test)
accuracy_ANN = 1 - result
print("Accuracy : {}".format(accuracy_ANN))

epochs_hist.history.keys()

plt.plot(epochs_hist.history['loss'])
plt.plot(epochs_hist.history['val_loss'])
plt.title('Model Loss Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training and Validation Loss')
plt.legend(['Training Loss', 'Validation Loss'])

y_predict = ANN_model.predict(X_test)
plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlabel('Model Predictions')
plt.ylabel('True Values')

y_predict_orig = scaler_y.inverse_transform(y_predict)
y_test_orig = scaler_y.inverse_transform(y_test)

plt.plot(y_test_orig, y_predict_orig, "^", color = 'r')
plt.xlabel('Model Predictions')
plt.ylabel('True Values')

k = X_test.shape[1]
n = len(X_test)
n

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)),'.3f'))
MSE = mean_squared_error(y_test_orig, y_predict_orig)
MAE = mean_absolute_error(y_test_orig, y_predict_orig)
r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)